# Personal Portfolio

Welcome to my GitHub portfolio! Data science is about posing questions and using data to get context, insight and perspective for making quick and thoughtful decision. While I enjoy applying data science everywhere, there is a special place in my heart for trust & safety, compliance, and privacy. 

This repository showcases my skills as both a data scientist and a problem solver. It's a living document, regularly updated as I develop new skills and complete projects, providing a comprehensive overview of my coding abilities and achievements.

## Introduction

My career intersects data science, trust & safety/compliance, and privacy. I lead and manage end-to-end projects, working with cross-functional teams to tackle local, regional, and global challenges. I graduate from my Masters in Data Science program (MIDS) at UC Berkeley in Spring 2025 and have specialized in NLP/ML. Here are some of the tools I use:

- **SQL**
- **Python**
- **Regex**
- **Graph Data Science**
- **Machine Learning**
- **Natural Language Processing**

## Repository Structure

Below is the structure of this repository, which includes various files and directories associated with my projects:

**personal_portfolio/**
  - **Dockerfile**: Configuration file for Docker, used to build Docker containers.
  - **docker-compose.yaml**: A YAML file for defining and running multi-container Docker applications. While using pip and anaconda are great, Docker is clean, easy to reproduce and when things go sideways, you can start fresh! 
  - **requirements.txt**: Lists all Python libraries required by the projects within the repo and used to pre-configure new Docker containers.
  - **README.md**: You are reading me right now ðŸ˜€
  - **src/**: Source code directory containing project-specific subdirectories.
    - **data/**: Raw data files used in projects.
    - **notebooks/**: The purpose of these notebooks is to demonstrate fundamental python and DS skills.
      - **a_b_test.ipynb**: Notebook detailing A/B testing procedures.
      - **api_request.ipynb**: How to use FDA's API for pulling recall data.
      - **lr_from_scratch**: Notebook demonstrating how to build a logistic regression model from scratch - great for anyone revisiting the basics of forward pass and backpropagation.
      - **nn_from_scratch**: Explains the construction of a neural network from the ground up (be sure to review lr_from_scratch first and use to compare)
      - **tfidf_from_scratch**: Illustrates how to implement TF-IDF from scratch in Python (just reminds you how lucky we are to have sklearn's tfidf_vectorizer, which I threw in at the end)


## Contact Me

Interested in collaborating or learning more about my work? Feel free to reach out!

- **Email**: [michael.kalish@berkeley.edu](mailto:michael.kalish@berkeley.edu)


